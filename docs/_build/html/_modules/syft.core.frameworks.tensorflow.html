

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="python" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="python" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>syft.core.frameworks.tensorflow package &mdash; PySyft 0.1 documentation</title>
  

  
  
  
  

  

  
  
    

  

  
    <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/PySyft_docs.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> PySyft
          

          
            
            <img src="../_static/logo.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                0.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../_autosummary/syft.core.frameworks.torch.html">syft.core.frameworks.torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../_autosummary/syft.core.utils.html">syft.core.utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="../_autosummary/syft.core.workers.html">syft.core.workers</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">PySyft</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>syft.core.frameworks.tensorflow package</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/_modules/syft.core.frameworks.tensorflow.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="syft-core-frameworks-tensorflow-package">
<h1>syft.core.frameworks.tensorflow package<a class="headerlink" href="#syft-core-frameworks-tensorflow-package" title="Permalink to this headline">¶</a></h1>
<div class="section" id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-syft.core.frameworks.tensorflow.federated_averaging_optimizer">
<span id="syft-core-frameworks-tensorflow-federated-averaging-optimizer-module"></span><h2>syft.core.frameworks.tensorflow.federated_averaging_optimizer module<a class="headerlink" href="#module-syft.core.frameworks.tensorflow.federated_averaging_optimizer" title="Permalink to this headline">¶</a></h2>
<p>Synchronize replicas for FedAvg training.</p>
<dl class="class">
<dt id="syft.core.frameworks.tensorflow.federated_averaging_optimizer.FederatedAveragingOptimizer">
<em class="property">class </em><code class="descclassname">syft.core.frameworks.tensorflow.federated_averaging_optimizer.</code><code class="descname">FederatedAveragingOptimizer</code><span class="sig-paren">(</span><em>opt</em>, <em>replicas_to_aggregate</em>, <em>interval_steps</em>, <em>total_num_replicas=None</em>, <em>device_setter=None</em>, <em>use_locking=False</em>, <em>name='fedAverage'</em><span class="sig-paren">)</span><a class="headerlink" href="#syft.core.frameworks.tensorflow.federated_averaging_optimizer.FederatedAveragingOptimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">tensorflow.python.training.optimizer.Optimizer</span></code></p>
<p>Class to synchronize and aggregate model params.</p>
<p>In a typical synchronous training environment, gradients will be averaged each
step and then applied to the variables in one shot, after which replicas can
fetch the new variables and continue. In a federated average training environment,
model variables will be averaged every &#8216;interval_steps&#8217; steps, and then the
replicas will fetch the new variables and continue training locally. In the
interval between two average operations, there is no data transfer, which can
accelerate training.</p>
<p>The following accumulators/queue are created:
&lt;empty line&gt;
* N <cite>parameter accumulators</cite>, one per variable to train. Local variables are
pushed to them and the chief worker will wait until enough variables are
collected and then average them. The accumulator will drop all stale variables
(more details in the accumulator op).
* 1 <cite>token</cite> queue where the optimizer pushes the new global_step value after</p>
<blockquote>
<div>all variables are updated.</div></blockquote>
<p>The following local variable is created:
* <cite>local_step</cite>, one per replica. Compared against the global_step in</p>
<blockquote>
<div>each accumulator to check for staleness of the variables.</div></blockquote>
<p>The optimizer adds nodes to the graph to collect local variables and pause
the trainers until variables are updated.
For the Parameter Server job:
&lt;empty line&gt;
1. An accumulator is created for each variable, and each replica pushes the</p>
<blockquote>
<div>local variables into the accumulators.</div></blockquote>
<ol class="arabic simple" start="2">
<li>Each accumulator averages once enough variables (replicas_to_aggregate)
have been accumulated.</li>
<li>Apply the averaged variables to global variables.</li>
<li>Only after all variables have been updated, increment the global step.</li>
<li>Only after step 4, pushes <cite>global_step</cite> in the <cite>token_queue</cite>, once for
each worker replica. The workers can now fetch the global step, use it to
update its local_step variable and start the next round.</li>
</ol>
<p>For the replicas:
&lt;empty line&gt;
1. Start a training block: fetch variables and train for &#8220;interval_steps&#8221; steps.
2. Once the training block has been computed, push local variables into variable</p>
<blockquote>
<div>accumulators. Each accumulator will check the staleness and drop the stale.</div></blockquote>
<ol class="arabic simple" start="3">
<li>After pushing all the variables, dequeue an updated value of global_step
from the token queue and record that step to its local_step variable. Note
that this is effectively a barrier.</li>
<li>Fetch new variables and start the next block.</li>
</ol>
<p>### Usage</p>
<p><a href="#id1"><span class="problematic" id="id2">``</span></a><a href="#id3"><span class="problematic" id="id4">`</span></a>python
# Create any optimizer to update the variables, say a simple SGD:
opt = GradientDescentOptimizer(learning_rate=0.1)</p>
<p># Wrap the optimizer with fed_avg_optimizer with 50 replicas: at each
# step the FederatedAveragingOptimizer collects &#8220;replicas_to_aggregate&#8221; variables
# before applying the average. Note that if you want to have 2 backup replicas,
# you can change total_num_replicas=52 and make sure this number matches how
# many physical replicas you started in your job.
opt = fed_avg_optimizer.FederatedAveragingOptimizer(opt,</p>
<blockquote>
<div>replicas_to_aggregate=50,
interval_steps=100,
device_setter)</div></blockquote>
<p># Some models have startup_delays to help stabilize the model but when using
# federated_average training, set it to 0.</p>
<p># Now you can call &#8216;minimize() normally&#8217;
# train_op = opt.minimize(loss, global_step=global_step)</p>
<p># And also, create the hook which handles initialization, queues and averaging.
fed_avg_hook = opt.make_session_run_hook(is_chief)
<a href="#id5"><span class="problematic" id="id6">``</span></a><a href="#id7"><span class="problematic" id="id8">`</span></a></p>
<p>In the training program, every worker will run the train_op as if not
averaged or synchronized. Note that if you want to run other ops like
test op, you should use common session instead of MonitoredSession:</p>
<p><a href="#id9"><span class="problematic" id="id10">``</span></a><a href="#id11"><span class="problematic" id="id12">`</span></a>python
with training.MonitoredTrainingSession(</p>
<blockquote>
<div><blockquote>
<div>master=workers[worker_id].target, is_chief=is_chief,
hooks=[ma_replicas_hook, ma_hook]) as mon_sess:</div></blockquote>
<dl class="docutils">
<dt>while not mon_sess.should_stop():</dt>
<dd>mon_sess.run(training_op)
sess = mon_sess._tf_sess()
sess.run(testing_op)</dd>
</dl>
</div></blockquote>
<p><a href="#id13"><span class="problematic" id="id14">``</span></a><a href="#id15"><span class="problematic" id="id16">`</span></a></p>
<dl class="method">
<dt id="syft.core.frameworks.tensorflow.federated_averaging_optimizer.FederatedAveragingOptimizer.get_chief_queue_runner">
<code class="descname">get_chief_queue_runner</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#syft.core.frameworks.tensorflow.federated_averaging_optimizer.FederatedAveragingOptimizer.get_chief_queue_runner" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the QueueRunner for the chief to execute.</p>
<p>This includes the operations to synchronize replicas: aggregate weights,
apply to variables, increment global step, insert tokens to token queue.</p>
<p>Note that this can only be called after calling apply_model_average() which
actually generates this queuerunner.</p>
<dl class="docutils">
<dt>Returns:</dt>
<dd>A <cite>QueueRunner</cite> for chief to execute.</dd>
<dt>Raises:</dt>
<dd>ValueError: If this is called before apply_model_average().</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="syft.core.frameworks.tensorflow.federated_averaging_optimizer.FederatedAveragingOptimizer.get_init_tokens_op">
<code class="descname">get_init_tokens_op</code><span class="sig-paren">(</span><em>num_tokens=-1</em><span class="sig-paren">)</span><a class="headerlink" href="#syft.core.frameworks.tensorflow.federated_averaging_optimizer.FederatedAveragingOptimizer.get_init_tokens_op" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the op to fill the sync_token_queue with the tokens.</p>
<p>This is supposed to be executed in the beginning of the chief/sync thread
so that even if the total_num_replicas is less than replicas_to_aggregate,
the model can still proceed as the replicas can compute multiple steps per
variable update. Make sure:
<cite>num_tokens &gt;= replicas_to_aggregate - total_num_replicas</cite>.</p>
<dl class="docutils">
<dt>Args:</dt>
<dd>num_tokens: Number of tokens to add to the queue.</dd>
<dt>Returns:</dt>
<dd>An op for the chief/sync replica to fill the token queue.</dd>
<dt>Raises:</dt>
<dd><p class="first">ValueError: If this is called before apply_model_average().
ValueError: If num_tokens are smaller than replicas_to_aggregate -</p>
<blockquote class="last">
<div>total_num_replicas.</div></blockquote>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="syft.core.frameworks.tensorflow.federated_averaging_optimizer.FederatedAveragingOptimizer.make_session_run_hook">
<code class="descname">make_session_run_hook</code><span class="sig-paren">(</span><em>is_chief</em>, <em>num_tokens=-1</em><span class="sig-paren">)</span><a class="headerlink" href="#syft.core.frameworks.tensorflow.federated_averaging_optimizer.FederatedAveragingOptimizer.make_session_run_hook" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a hook to handle federated average and init operations.</p>
</dd></dl>

<dl class="method">
<dt id="syft.core.frameworks.tensorflow.federated_averaging_optimizer.FederatedAveragingOptimizer.minimize">
<code class="descname">minimize</code><span class="sig-paren">(</span><em>loss</em>, <em>global_step=None</em><span class="sig-paren">)</span><a class="headerlink" href="#syft.core.frameworks.tensorflow.federated_averaging_optimizer.FederatedAveragingOptimizer.minimize" title="Permalink to this definition">¶</a></dt>
<dd><p>Add operations to minimize <cite>loss</cite> by updating <cite>var_list</cite>.
This simply wraps the minimize() from the real optimizer.
Args:</p>
<blockquote>
<div><p>loss: A <cite>Tensor</cite> containing the value to minimize.
global_step: Optional <cite>Variable</cite> to increment by one after the</p>
<blockquote>
<div>variables have been updated.</div></blockquote>
</div></blockquote>
<dl class="docutils">
<dt>Returns:</dt>
<dd>An Operation that updates the variables in <cite>var_list</cite>.  If <cite>global_step</cite>
was not <cite>None</cite>, that operation also increments <cite>global_step</cite>.</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-syft.core.frameworks.tensorflow">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-syft.core.frameworks.tensorflow" title="Permalink to this headline">¶</a></h2>
</div>
</div>


           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, OpenMined Contributors.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'0.1',
            LANGUAGE:'python',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="../_static/js/theme.js"></script>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>